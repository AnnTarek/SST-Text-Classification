{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
      "        num_rows: 8544\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
      "        num_rows: 1101\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
      "        num_rows: 2210\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"sst\", \"default\") #load dataset \n",
    "#Familiarize with the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': [\"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\"], 'label': [0.6944400072097778], 'tokens': [\"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\"], 'tree': ['70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0']}\n"
     ]
    }
   ],
   "source": [
    "#print first example of the training set\n",
    "print(dataset['train'][:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", \"The|gorgeously|elaborate|continuation|of|``|The|Lord|of|the|Rings|''|trilogy|is|so|huge|that|a|column|of|words|can|not|adequately|describe|co-writer\\\\/director|Peter|Jackson|'s|expanded|vision|of|J.R.R.|Tolkien|'s|Middle-earth|.\", 'Singer\\\\/composer|Bryan|Adams|contributes|a|slew|of|songs|--|a|few|potential|hits|,|a|few|more|simply|intrusive|to|the|story|--|but|the|whole|package|certainly|captures|the|intended|,|er|,|spirit|of|the|piece|.', \"You|'d|think|by|now|America|would|have|had|enough|of|plucky|British|eccentrics|with|hearts|of|gold|.\", 'Yet|the|act|is|still|charming|here|.']\n",
      "[\"It|'s|a|lovely|film|with|lovely|performances|by|Buy|and|Accorsi|.\", 'No|one|goes|unindicted|here|,|which|is|probably|for|the|best|.', \"And|if|you|'re|not|nearly|moved|to|tears|by|a|couple|of|scenes|,|you|'ve|got|ice|water|in|your|veins|.\", 'A|warm|,|funny|,|engaging|film|.', 'Uses|sharp|humor|and|insight|into|human|nature|to|examine|class|conflict|,|adolescent|yearning|,|the|roots|of|friendship|and|sexual|identity|.']\n",
      "['Effective|but|too-tepid|biopic', 'If|you|sometimes|like|to|go|to|the|movies|to|have|fun|,|Wasabi|is|a|good|place|to|start|.', \"Emerges|as|something|rare|,|an|issue|movie|that|'s|so|honest|and|keenly|observed|that|it|does|n't|feel|like|one|.\", 'The|film|provides|some|great|insight|into|the|neurotic|mindset|of|all|comics|--|even|those|who|have|reached|the|absolute|top|of|the|game|.', 'Offers|that|rare|combination|of|entertainment|and|education|.']\n"
     ]
    }
   ],
   "source": [
    "#splitting the dataset into training, val, and testing sets\n",
    "train = {\n",
    "    'sentence': dataset['train']['sentence'],\n",
    "    'label': dataset['train']['label'],\n",
    "    'tokens': dataset['train']['tokens']\n",
    "}\n",
    "val = {\n",
    "    'sentence': dataset['validation']['sentence'],\n",
    "    'label': dataset['validation']['label'],\n",
    "    'tokens': dataset['validation']['tokens']\n",
    "}\n",
    "test = {\n",
    "    'sentence': dataset['test']['sentence'],\n",
    "    'label': dataset['test']['label'],\n",
    "    'tokens': dataset['test']['tokens']\n",
    "}\n",
    "\n",
    "print(train['tokens'][:5])\n",
    "print(val['tokens'][:5])\n",
    "print(test['tokens'][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6944400072097778\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#separating the data into classes\n",
    "print(train['label'][0])\n",
    "for i in range(len(train['sentence'])):\n",
    "    if train['label'][i] >= 0 and train['label'][i] <= 0.2:\n",
    "        train['label'][i] = 0\n",
    "    elif train['label'][i] > 0.2 and train['label'][i] <= 0.4:\n",
    "        train['label'][i] = 1\n",
    "    elif train['label'][i] > 0.4 and train['label'][i] <= 0.6:\n",
    "        train['label'][i] = 2\n",
    "    elif train['label'][i] > 0.6 and train['label'][i] <= 0.8:\n",
    "        train['label'][i] = 3\n",
    "    elif train['label'][i] > 0.8 and train['label'][i] <= 1:\n",
    "        train['label'][i] = 4\n",
    "print(train['label'][0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitc Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Feature Representation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'Rock') 8\n",
      "('Rock', 'is') 1\n",
      "('is', 'destined') 1\n",
      "('destined', 'to') 4\n",
      "('to', 'be') 243\n",
      "('be', 'the') 28\n",
      "('the', '21st') 3\n",
      "('21st', 'Century') 2\n",
      "('Century', \"'s\") 1\n",
      "(\"'s\", 'new') 4\n",
      "\n",
      "Length: 87247\n"
     ]
    }
   ],
   "source": [
    "#exploring bigrams in dataset\n",
    "bigrams = {}\n",
    "for sentence in train['tokens']:\n",
    "    words = sentence.split('|')\n",
    "    for w1, w2 in zip(words[:-1], words[1:]):\n",
    "        if (w1, w2) in bigrams:\n",
    "            bigrams[(w1, w2)] += 1\n",
    "        else:\n",
    "            bigrams[(w1, w2)] = 1\n",
    "        \n",
    "#print first 10 bigrams\n",
    "count = 0\n",
    "for key, value in bigrams.items():\n",
    "    print(key, value)\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        break\n",
    "\n",
    "print(f\"\\nLength: {len(bigrams)}\") #number of bigrams\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "2\n",
      "8544\n"
     ]
    }
   ],
   "source": [
    "print(bigrams[('a', 'solid')]) #frequency of bigram ('a', 'solid')\n",
    "print(bigrams[('21st', 'Century')]) #frequency of bigram ('a', 'rock')\n",
    "\n",
    "print(len(train['tokens'])) #number of sentences in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
